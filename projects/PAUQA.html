<!DOCTYPE html>
<html lang="en">


<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="CCMSRNet">

    <title>A multi-task segmentation and classification network for remote ship hull inspection</title>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/charts.css/dist/charts.min.css">
    <link id="theme-style" rel="stylesheet" href="./Packages/SSSN_FSS/css/avatarclip_main.css">
    <link id="theme-style" rel="stylesheet" href="./Packages/SSSN_FSS/css/bulma-carousel.min.css">
    <link id="theme-style" rel="stylesheet" href="./Packages/SSSN_FSS/css/bulma-slider.min.css">

    <!-- <script type="module" src="./assets/js/background_box.js"></script> -->
    <script type="module" src="./js/background_star.js"></script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="./Packages/SSSN_FSS/js/bulma-carousel.min.js"></script>
    <script src="./Packages/SSSN_FSS/js/bulma-slider.min.js"></script>
    <script src="./Packages/SSSN_FSS/js/index.js"></script>
    <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>
</head>

<body>
    
    
    <div class="wrapper">
        <!--
        <section class="section">
        
            <div class="details">
                <div align="center">
                <img class='links-cover'
                        src='./Data/banner/banner_test.png' ,height="50%" width="100%"></div>
            </div>
            
            
        </section>
        -->
        <section class="section intro-section">
            <div class="intro-container" style="text-align: center;">
                <div class="header">
                    <h3 class="papername">Perception-Aware Underwater Image Quality Assessment: Data Set, Perceptual Quality Scores and Assessment Network</h3>
                </div>
                <ul class="list-unstyled name-list">
                    <li><a href="https://www.researchgate.net/profile/Bosen-Lin" target="_blank">Bosen </a></li>
                    <li><a href="https://scholar.google.com/citations?user=InD0J_IAAAAJ&hl=en&oi=ao" target="_blank">Xinghui Dong</a></li>
    
                </ul>
                <ul class="list-unstyled name-list">
                    <li><a href="https://indtlab.github.io/" target="_blank">INDTLab, Ocean University of China </a></li>
                </ul>
                <!-- <ul class="list-unstyled name-list">
                    <li><a href="" target="_blank">Ocean University of China</a></li>
                </ul> -->

            </div>
            </br>
       

        <div align="center">
        <div class="container">
            <div class="columns is-multiline is-centered">
              <table>
                <tr>
                  <!-- <td style="padding:5px 5px 5px 5px;">
                      <img src='./Packages/ShipHullSurvey/Data/Figure6.png' height="500" width="889" />
                  </td> -->
                  <!-- <td style="padding:5px 5px 5px 5px;">
                      <img src='./Packages/BracketFlare/Data/removal_result/Fig6_input2_000001.png' onmouseover="this.src='./Packages/BracketFlare/Data/removal_result/Fig6_output2_000001.png';" onmouseout="this.src='./Packages/BracketFlare/Data/removal_result/Fig6_input2_000001.png';"  height="256" width="256" />
                  </td>
                  <td style="padding:5px 5px 5px 5px;">
                      <img src='./Packages/BracketFlare/Data/removal_result/Fig6_input2_000002.png' onmouseover="this.src='./Packages/BracketFlare/Data/removal_result/Fig6_output2_000002.png';" onmouseout="this.src='./Packages/BracketFlare/Data/removal_result/Fig6_input2_000002.png';"  height="256" width="256" />
                  </td>
                  <td style="padding:5px 5px 5px 5px;">
                      <img src='./Packages/BracketFlare/Data/removal_result/Fig6_input2_000003.png' onmouseover="this.src='./Packages/BracketFlare/Data/removal_result/Fig6_output2_000003.png';" onmouseout="this.src='./Packages/BracketFlare/Data/removal_result/Fig6_input2_000003.png';"  height="256" width="256" />
                  </td>	 -->
                </tr>
                <!-- <tr>
                    <td style="padding:5px 5px 5px 5px;">
                        <img src='./Packages/BracketFlare/Data/removal_result/Fig6_input2_000004.png' onmouseover="this.src='./Packages/BracketFlare/Data/removal_result/Fig6_output2_000004.png';" onmouseout="this.src='./Packages/BracketFlare/Data/removal_result/Fig6_input2_000004.png';"  height="256" width="256" />
                    </td>
                    <td style="padding:5px 5px 5px 5px;">
                        <img src='./Packages/BracketFlare/Data/removal_result/Fig6_input2_000005.png' onmouseover="this.src='./Packages/BracketFlare/Data/removal_result/Fig6_output2_000005.png';" onmouseout="this.src='./Packages/BracketFlare/Data/removal_result/Fig6_input2_000005.png';"  height="256" width="256" />
                    </td>
                    <td style="padding:5px 5px 5px 5px;">
                        <img src='./Packages/BracketFlare/Data/removal_result/Fig6_input2_000006.png' onmouseover="this.src='./Packages/BracketFlare/Data/removal_result/Fig6_output2_000006.png';" onmouseout="this.src='./Packages/BracketFlare/Data/removal_result/Fig6_input2_000006.png';"  height="256" width="256" />
                    </td>
                    <td style="padding:5px 5px 5px 5px;">
                        <img src='./Packages/BracketFlare/Data/removal_result/Fig6_input2_000000.png' onmouseover="this.src='./Packages/BracketFlare/Data/removal_result/Fig6_output2_000000.png';" onmouseout="this.src='./Packages/BracketFlare/Data/removal_result/Fig6_input2_000000.png';"  height="256" width="256" />
                    </td>	
                  </tr> -->
                  
              </table>
            </div>
          </div>
            <!-- <p>  The diagram of the inspection of a ship hull using robots, including the Automated Underwater Vehicle (AUV), Remotely Operate Vehicle (ROV), Unmanned Aerial Vehicle (UAV) and climbing robot.</p>             -->
            
        </div>
    </section>

        <section class='section'>
            <div class="section-title">
                Abstract
            </div>
            <div class="details" style="text-align: justify" ;>
                Underwater Image Quality Assessment (UIQA) plays an important role in assess the effectiveness of Underwater Image Enhancement (UIE) algorithms or to evaluate the quality of underwater images. 
                However, accurate UIQA that are consistent with human perception remains challenging. This dilemma on one hand is attributed to the lack of real human visual perception UIQA data, and on the other
                hand that the quality feature representation used by exist- ing UIQA algorithms are inconsistent with human perceptions. To address these issues, we introduce a Large scale Underwater Image Quality
                Data set (LUIQD) which acquires human perception image quality score, and propose an UIQA net- work based on an efficient visual Transformer network with color and sharpness assessment. Specifically, 
                the LUIQD includes 64,180 real underwater images and enhance images covering a wide range of scenes, target and imaging conditions. Subjective user studies are designed to annotate and evaluate the
                perceptual quality score of the images in LUIQD. Based on the analysis of the mech- anisms of human perception, we further design a data-driven UIQA network that integrates an efficient convolutional 
                attention vision transformer to extract multi-scale features by a multi-path structure. Considering the specificity of human perception of underwater images, features from the chrominance and luminance 
                domains are extracted and fused with local and global images features for joint feature interaction. This UIQA network is named as Perception-Aware Underwater image Quality Assessment Network 
                (PAUQA-Net). Extensive exper- iments conduted on LUIQD and other data sets demonstrate that the proposed PAUQA-Net achieves superior assessment performance compared with the most popular UIQA and IQA
                methods.
            </div>
        </section>


        <!-- <section class='section'>
            <div class="section-title">
                The multi-task segmentation and classification hull inspection network
            </div>
            <div class="details" style="text-align: justify" ;>
                This section introduces the proposed MTHI-Net, a multi-task segmentation and classification network designed for ship hull inspection. The architecture of MTHI-Net starts with a 
              stack encoder–decoder network. Two self-attention modules are used to enhance feature capture: the DAM and the RAM. Additionally, a RRM is employed to refine the segmentation outputs. 
              Motivated by the multi-task learning parametersharing approach, the FFM is used to fuse feature maps for defect classification. A segmentation mask is used to narrow the FOV to the 
              defect regions. Finally, the proposed MTHI-Net undergoes end-to-end training to achieve simultaneous defect classification and segmentation by combining the single-task loss of the 
              segmentation network and classifier. In addition, a lightweight MTHI-Net called MTHI-Net-Lite is introduced.
            </div>

          <div align="center">
                <div class="container">
                    <div class="columns is-multiline is-centered">
                      <table>
                        <tr>
                          <td style="padding:5px 5px 5px 5px;">
                              <img src='./Packages/linMultitaskSegmentationClassification2024/Data/Figures_rit.png' height="500" width="889" />
                          </td>
                        </tr>
                      </table>
                    </div>
                  </div>
                    <p>  The flowchart of the remote ship hull inspection using the proposed multi-task classification and segmentation network MTHI-Net. For the hull image collected by the remote inspection 
                      robot, MTHI-Net on the cloud server obtained classification data for general defect type and segmentation mask for defect location and quantification. The inspector finally draws conclusions 
                      on hull condition.</p>            
                </div>

            <div class="details" style="text-align: justify" ;>
                MTHI-Net comprises three parts: (i) a stack encoder–decoder network with self-attention modules for multi-resolution feature extraction and defect segmentation; (ii) a RRM for segmentation; and 
              (iii) a feature fusion network following a classifier for defect classification. The proposed network uses RGB images of ship hulls as input and performs defect segmentation and classification tasks 
              simultaneously.
            </div>

    <div align="center">
                <div class="container">
                    <div class="columns is-multiline is-centered">
                      <table>
                        <tr>
                          <td style="padding:5px 5px 5px 5px;">
                              <img src='./Packages/linMultitaskSegmentationClassification2024/Data/Figures_overall.png' height="500" width="889" />
                          </td>
                        </tr>
                      </table>
                    </div>
                  </div>
                    <p>  The network architecture of the proposed multi-task segmentation and classification network MTHI-Net. The number represents the shape of the feature map or the number of neurons.</p>            
                </div>
          
        </section>

      <section class='section'>
            <div class="section-title">
                Experimental results
            </div>
            <div class="details" style="text-align: justify" ;>
                Benefiting from the multi-task learning mechanism, the proposed MTHI-Net and MTHI-Net-Lite models can use defect categories as auxiliary information when performing defect segmentation tasks. 
              This feature exchange enhances the capacity of the network to extract meaningful and separable features, leading to improved performance in hull defect segmentation tasks compared to single-task 
              methods. The spatial and inter-channel attention mechanism further guides the network towards defective areas with similar characteristics. Therefore, the proposed MTHI-Net excelled at extracting 
              defect features and provided better segmentation outcomes.
            </div>

        
          <div align="center">
                <div class="container">
                    <div class="columns is-multiline is-centered">
                      <table>
                        <tr>
                          <td style="padding:5px 5px 5px 5px;">
                              <img src='./Packages/linMultitaskSegmentationClassification2024/Data/Figures_hires.png' height="500" width="889" />
                          </td>
                        </tr>
                      </table>
                    </div>
                  </div>
                    <p>  The defect segmentation results obtained on MaVeCoDD-HiRes data set. Each column shows the results derived using (a) U-Net, (b) X-Net, (c) Trans U-Net, (d) Swin U-Net, 
                      (e) MTHI-Net (f) MTHI-Net-Lite, and (g) the Ground Truth in turn. The type of defect is shown on the left side of each row.</p>            
                </div>
        
          <div align="center">
                <div class="container">
                    <div class="columns is-multiline is-centered">
                      <table>
                        <tr>
                          <td style="padding:5px 5px 5px 5px;">
                              <img src='./Packages/linMultitaskSegmentationClassification2024/Data/Figures_lores.png' height="500" width="889" />
                          </td>
                        </tr>
                      </table>
                    </div>
                  </div>
                    <p>  The defect segmentation results obtained on MaVeCoDD-LoRes data set. Each column shows the results derived using (a) U-Net, (b) X-Net, (c) Trans U-Net, (d) Swin U-Net, 
                      (e) MTHI-Net (f) MTHI-Net-Lite, and (g) the Ground Truth in turn. The type of defect is shown on the left side of each row.</p>            
                </div>

            <div class="details" style="text-align: justify" ;>
                Unlike existing methods that directly stack convolutional layers, the proposed model uses spatial and inter-channel attention modules to extract features from different stack encoders of 
              the segmentation branch and subsequently filters these features using a segmented mask. By leveraging this mechanism, the proposed multi-task networks prioritised more relevant and informative 
              features, improving the overall performance of defect classification, even in feature-limited underwater environments.
            </div>
        
            <div align="center">
                <div class="container">
                    <div class="columns is-multiline is-centered">
                      <table>
                        <tr>
                          <td style="padding:5px 5px 5px 5px;">
                              <img src='./Packages/linMultitaskSegmentationClassification2024/Data/Figures_liaci.png' height="500" width="889" />
                          </td>
                        </tr>
                      </table>
                    </div>
                  </div>
                    <p>  The defect segmentation results obtained on LIACi data set. Each column shows the results derived using (a) U-Net, (b) X-Net, (c) Trans U-Net, (d) Swin U-Net, 
                      (e) MTHI-Net (f) MTHI-Net-Lite, and (g) the Ground Truth in turn. The type of defect is shown on the left side of each row.</p>            
                </div>
        
        </section>

        <section class='section'>
            <div class="section-title">
                Conclusion
            </div>
            <div class="details" style="text-align: justify" ;>
                Automated image processing methods have the potential to reduce the reliance on manual inspection and improve detection efficiency and accuracy. In this study, we proposed 
              a novel multitasking ship inspection network, MTHI-Net, designed for synchronous ship-hull defect segmentation and classification. The MTHI-Net architecture is realised through 
              a stacked encoder–decoder network with self-attention mechanisms to enhance feature extraction capabilities. To address missegmentation issues, an RRM was introduced. For defect
              classification, the FFM fuses features from different layers of the stacked network. The network is trained end-to-end using a multi-task loss function.
              A lightweight version of the network, MTHI-Net-Lite, provides a solution for deployment on edge devices. Extensive experiments were conducted to analyse the benefits of the proposed 
              MTHI-Net in the reconstructed MaVeCoDD and LIACi ship-hull inspection datasets. The results demonstrate that the proposed MTHI-Net outperformed the other approaches in both tasks. 
              The lightweight version of the network enables competitive inspections.
            </div>
            
        </section> -->

      
        
        <section class='section links-section'>
            <div class='section-title'>
                Links
            </div>
            <div class='details links-table'>
                <table>
                    <tr>
                        <!-- <td>
                            <div class='links-container'>
                                <a href='https://www.sciencedirect.com/science/article/pii/S0029801824009454' target="_blank"><img class='links-cover'src='./Packages/ShipHullSurvey/Data/Cover.png' alt='PDF Cover' width="140"></a>
                            </div>
                        </td> -->
                        <!-- <td>
                            <div class='links-container'>
                                <a href='https://github.com/INDTLab/SSSN_FSS' target="_blank"><img class='links-cover'
                                        src='./Packages/CCMSRNet/Data/github.png' alt='github icon' width="160"></a>
                            </div>
                        </td> -->
                    </tr>
                    
                    <tr>
                        <td>
                            Paper
                        </td>
                        
                        <td>
                            <a href='https://github.com/CatchACat083/PAUQA' target="_blank"> Code </a>
                        </td>
                        
                    </tr>
                    
                </table>
            </div>
        </section>
        

       
        <!-- <section class="section">
            <div class="section-title">
                Video
            </div>
            <div class="details">
                    <center><iframe width="1080" height="608" src="https://www.youtube.com/embed/FM8kAM13zUA" title="YouTube video player" 
                    frameborder="0" 
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                    allowfullscreen></iframe></center>
            </div>
        </section> -->

        <!-- <section class="section">
            <div class="section-title">
                Dataset
            </div>
            <div class="details">
                <div align="center">
                <img class='links-cover'
                        src='./Packages/BracketFlare/Data/piechart_screenshot.png' ,height="50%" width="80%"></div>
            </div>
            <p> Visualization of typical examples and distribution of our dataset. The dataset provides different types of light sources in diverse scenes. 
                Based on light source types, we classify them into eight categories with different flare patterns.</p>
        </section> -->
        
        <!-- <section class="section">
            <div class="section-title">
                Architecture
            </div>
            <div class="details">
                <div align="center">
                <img class='links-cover'
                        src='./Packages/SSSN_FSS/Data/network/arch.jpg' ,height="50%" width="80%"></div>
            </div>
            
        </section>

        <section class="section">
            <div class="section-title">
                Experimental Results
            </div>
            <div align="center">
                <div class="container">
                    <div class="columns is-multiline is-centered">
                      <table>
                        <tr>
                          <td style="padding:5px 5px 5px 5px;">
                            <img src='./Packages/SSSN_FSS/Data/results/Table.png' height="80%" width="100%" />
                        </td>
                        </tr>
                         
                        
                          
                      </table>
                    </div>
                  </div>
                     <p> <b>Quantitative Results on UIEB, SUIM-E, EUVP and RUIE data sets.</b></p>             -->
                    
                <!-- </div>
            
                <div align="center">
                    <div class="container">
                        <div class="columns is-multiline is-centered">
                          <table>
                            <tr>
                              <td style="padding:5px 5px 5px 5px;">
                                  <img src='./Packages/SSSN_FSS/Data/results/qualitative.jpg' height="80%" width="100%" />
                              </td>
                            </tr>

                            
                            
            
                             
                            
                              
                          </table>
                        </div>
                      </div>
                        <p> <b> Qualitative Results on PASCAL-5 data set.</b> </p>            
                        
                    </div>
            <div class="section-title">
                Lightweight Axial Self-Attention
            </div>
            <div class="details">
                <div align="center">
                <img class='links-cover'
                        src='./Packages/CCMSRNet/Data/network/attention_comparison.drawio.png' ,height="50%" width="80%"></div>
            </div> -->
        <!-- </section> --> 
        

        <section>
    
<!--             <p>We referred to the project page of <a href="https://ykdai.github.io/projects/BracketFlare">BracketFlare</a> when creating this
                project page.</p> -->
            <!-- <p> This project is licensed under <a
                href="https://github.com/ykdai/BracketFlare/blob/main/LICENSE">NTU S-Lab License 1.0</a>. Redistribution and use should follow this license.</p> -->
        </section>

    </div>


</body>

</html>
